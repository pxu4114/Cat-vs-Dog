{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, train_test_split\n",
    "from scipy.misc import imread\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Labels for dataset: 0 for cat and 1 for dog "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 1)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "labels_cat = np.zeros(12500)\n",
    "labels_dog = np.ones(12500)\n",
    "\n",
    "labels = np.concatenate([labels_cat,labels_dog]).reshape((-1,1))\n",
    "labels = np.asarray(labels).astype('float32')\n",
    "labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seperating cat and dog images from dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_files = glob.glob(\"train/train/cat*\")\n",
    "dog_files = glob.glob(\"train/train/dog*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_path_to_data(image_path):\n",
    "    image = imread(image_path)\n",
    "    #converting all image shape to have same size so that it concatenates in the end\n",
    "    image = np.resize(image,[100,100,3])\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:2: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "cat_vector = []\n",
    "for i in range(len(cat_files)):\n",
    "    cat_vector.append(convert_path_to_data(cat_files[i]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:2: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "dog_vector = []\n",
    "for i in range(len(dog_files)):\n",
    "    dog_vector.append(convert_path_to_data(dog_files[i]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 100, 3)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dog_vector[10].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.concatenate([cat_vector, dog_vector])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 100, 100, 3)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spliting data into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, data_test, labels_train, labels_test = train_test_split(X, labels, test_size=0.1, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 100, 100, 3)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and network parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.0001\n",
    "num_steps = 1000\n",
    "batch_size = 32\n",
    "num_classes = 1 \n",
    "dropout = 0.25 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_net(x_dict, n_classes, dropout, reuse, is_training):\n",
    "    \n",
    "    with tf.variable_scope('ConvNet', reuse=reuse):\n",
    "        # TF Estimator input is a dict, in case of multiple inputs\n",
    "        x = x_dict['images']\n",
    "        \n",
    "        conv1 = tf.layers.conv2d(x, 32, 3, activation=tf.nn.relu)\n",
    "        conv1 = tf.layers.max_pooling2d(conv1, 2, 2)\n",
    "        \n",
    "        conv2 = tf.layers.conv2d(conv1, 64, 3, activation=tf.nn.relu)\n",
    "        conv2 = tf.layers.max_pooling2d(conv2, 2, 2)\n",
    "        \n",
    "        conv3 = tf.layers.conv2d(conv2, 64, 3, activation=tf.nn.relu)\n",
    "        conv3 = tf.layers.max_pooling2d(conv3, 2, 2)\n",
    "\n",
    "        fc1 = tf.contrib.layers.flatten(conv3)\n",
    "        fc1 = tf.layers.dense(fc1, 1024)\n",
    "\n",
    "        fc2 = tf.layers.dropout(fc1, rate=dropout, training=is_training)\n",
    "\n",
    "        # Output layer, class prediction\n",
    "        out = tf.layers.dense(fc1, n_classes)\n",
    "        return out "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used adagradoptimizer for optimization. I tried with adamoptimizer but loss overshoots and produces Nan value after some point. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fn(features, labels, mode):\n",
    "    # Because Dropout have different behavior at training and prediction time, we\n",
    "    # need to create 2 distinct computation graphs that still share the same weights.\n",
    "    logits_train = conv_net(features, num_classes, dropout, reuse=False, is_training=True)\n",
    "    logits_test = conv_net(features, num_classes, dropout, reuse=True, is_training=False)\n",
    "\n",
    "    # Predictions\n",
    "    pred_classes = tf.argmax(logits_test, axis=1)\n",
    "    pred_probas = tf.nn.sigmoid(logits_test)\n",
    "    \n",
    "    # If prediction mode, early return\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(mode, predictions=pred_classes) \n",
    "        \n",
    "    # Define loss and optimizer\n",
    "    loss_op = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "        logits=logits_train, labels = tf.cast(labels, dtype=tf.float32)))\n",
    "    optimizer = tf.train.AdagradOptimizer(learning_rate=learning_rate)\n",
    "    train_op = optimizer.minimize(loss_op, global_step=tf.train.get_global_step())\n",
    "    #train_op = tf.keras.optimizers.SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    \n",
    "    # Evaluate the accuracy of the model\n",
    "    acc_op = tf.metrics.accuracy(labels=labels, predictions=pred_classes)\n",
    "\n",
    "    estim_specs = tf.estimator.EstimatorSpec(\n",
    "      mode=mode,\n",
    "      predictions=pred_classes,\n",
    "      loss=loss_op,\n",
    "      train_op=train_op,\n",
    "      eval_metric_ops={'accuracy': acc_op})\n",
    "\n",
    "    return estim_specs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpt5w4girh\n",
      "INFO:tensorflow:Using config: {'_save_checkpoints_secs': 600, '_log_step_count_steps': 100, '_tf_random_seed': None, '_model_dir': '/tmp/tmpt5w4girh', '_keep_checkpoint_max': 5, '_num_ps_replicas': 0, '_is_chief': True, '_keep_checkpoint_every_n_hours': 10000, '_session_config': None, '_task_type': 'worker', '_service': None, '_save_checkpoints_steps': None, '_save_summary_steps': 100, '_master': '', '_task_id': 0, '_evaluation_master': '', '_global_id_in_cluster': 0, '_num_worker_replicas': 1, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f19500f4b00>}\n"
     ]
    }
   ],
   "source": [
    "model = tf.estimator.Estimator(model_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting data into float32 datatype because tensorflow does not allow int  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_train = np.array(data_train, dtype=float32)\n",
    "data_train = data_train.astype(np.float32)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = data_test.astype(np.float32) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the function for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into /tmp/tmpt5w4girh/model.ckpt.\n",
      "INFO:tensorflow:step = 0, loss = 7.1603565\n",
      "INFO:tensorflow:global_step/sec: 34.4662\n",
      "INFO:tensorflow:step = 100, loss = 4.097828 (2.903 sec)\n",
      "INFO:tensorflow:global_step/sec: 36.33\n",
      "INFO:tensorflow:step = 200, loss = 2.3863828 (2.753 sec)\n",
      "INFO:tensorflow:global_step/sec: 36.4685\n",
      "INFO:tensorflow:step = 300, loss = 1.2911124 (2.742 sec)\n",
      "INFO:tensorflow:global_step/sec: 36.3772\n",
      "INFO:tensorflow:step = 400, loss = 3.6303272 (2.749 sec)\n",
      "INFO:tensorflow:global_step/sec: 36.3414\n",
      "INFO:tensorflow:step = 500, loss = 1.2282616 (2.752 sec)\n",
      "INFO:tensorflow:global_step/sec: 36.339\n",
      "INFO:tensorflow:step = 600, loss = 1.5382468 (2.752 sec)\n",
      "INFO:tensorflow:global_step/sec: 36.4486\n",
      "INFO:tensorflow:step = 700, loss = 1.8633181 (2.743 sec)\n",
      "INFO:tensorflow:global_step/sec: 36.3352\n",
      "INFO:tensorflow:step = 800, loss = 2.333419 (2.752 sec)\n",
      "INFO:tensorflow:global_step/sec: 36.6186\n",
      "INFO:tensorflow:step = 900, loss = 0.8606962 (2.730 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1000 into /tmp/tmpt5w4girh/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 1.8635385.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.estimator.estimator.Estimator at 0x7f192c2b0128>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={'images': data_train}, y = labels_train,\n",
    "    batch_size = batch_size, num_epochs = None, shuffle=True)\n",
    "# Train the Model\n",
    "model.train(input_fn, steps=num_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating the model performance on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-06-19-15:53:40\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpt5w4girh/model.ckpt-1000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-06-19-15:53:41\n",
      "INFO:tensorflow:Saving dict for global step 1000: accuracy = 0.506, global_step = 1000, loss = 1.4775387\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.506, 'global_step': 1000, 'loss': 1.4775387}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={'images': data_test}, y=labels_test,\n",
    "    batch_size=batch_size, shuffle=False)\n",
    "model.evaluate(input_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
